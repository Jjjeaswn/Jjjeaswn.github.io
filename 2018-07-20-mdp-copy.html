<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="Chinese">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Part III 强化学习与控制 我们现在开始学习强化学习和适应性控制。  在监督学习上，我们的算法尝试去让它们的输出跟训练集中的标签 $y​$ 相似。对于每个 $x​$，标签给出了一个无偏差的“标准答案”。但在很多序列决策和控制问题上，很难提供这样一个显式的监督给学习算法。比如，我们弄了个四条腿的机器人，想尝试编程让它走起来，但起初我们对什么是“正确”的动作，哪些动作能让它走起来没有任何想法，自">
<meta property="og:type" content="website">
<meta property="og:title" content="翻译：强化学习与控制">
<meta property="og:url" content="http://yoursite.com/2018-07-20-mdp-copy.html">
<meta property="og:site_name" content="Jjjeaswn&#39;s blog">
<meta property="og:description" content="Part III 强化学习与控制 我们现在开始学习强化学习和适应性控制。  在监督学习上，我们的算法尝试去让它们的输出跟训练集中的标签 $y​$ 相似。对于每个 $x​$，标签给出了一个无偏差的“标准答案”。但在很多序列决策和控制问题上，很难提供这样一个显式的监督给学习算法。比如，我们弄了个四条腿的机器人，想尝试编程让它走起来，但起初我们对什么是“正确”的动作，哪些动作能让它走起来没有任何想法，自">
<meta property="og:locale" content="Chinese">
<meta property="og:image" content="http://yoursite.com/assets/mdp_img1.png">
<meta property="og:image" content="http://yoursite.com/assets/mdp_img2.png">
<meta property="og:image" content="http://yoursite.com/assets/mdp_img3.png">
<meta property="og:image" content="http://yoursite.com/assets/mdp_img4.png">
<meta property="og:updated_time" content="2018-08-22T03:21:28.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="翻译：强化学习与控制">
<meta name="twitter:description" content="Part III 强化学习与控制 我们现在开始学习强化学习和适应性控制。  在监督学习上，我们的算法尝试去让它们的输出跟训练集中的标签 $y​$ 相似。对于每个 $x​$，标签给出了一个无偏差的“标准答案”。但在很多序列决策和控制问题上，很难提供这样一个显式的监督给学习算法。比如，我们弄了个四条腿的机器人，想尝试编程让它走起来，但起初我们对什么是“正确”的动作，哪些动作能让它走起来没有任何想法，自">
<meta name="twitter:image" content="http://yoursite.com/assets/mdp_img1.png">






  <link rel="canonical" href="http://yoursite.com/2018-07-20-mdp-copy.html"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>翻译：强化学习与控制 | Jjjeaswn's blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="Chinese">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jjjeaswn's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018-07-20-mdp-copy.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jjjeaswn Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jjjeaswn's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">翻译：强化学习与控制
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-20 11:13:00" itemprop="dateCreated datePublished" datetime="2018-08-20T11:13:00+08:00">2018-08-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-22 11:21:28" itemprop="dateModified" datetime="2018-08-22T11:21:28+08:00">2018-08-22</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="part-iii-强化学习与控制">Part III 强化学习与控制</h1>
<p>我们现在开始学习强化学习和适应性控制。</p>
<pre><code> 在监督学习上，我们的算法尝试去让它们的输出跟训练集中的标签 $y​$ 相似。对于每个 $x​$，标签给出了一个无偏差的“标准答案”。但在很多序列决策和控制问题上，很难提供这样一个显式的监督给学习算法。比如，我们弄了个四条腿的机器人，想尝试编程让它走起来，但起初我们对什么是“正确”的动作，哪些动作能让它走起来没有任何想法，自然而然无法提供给算法显式的监督让它学会行走。

 在强化学习框架，我们反过来只提供给我们的算法一个奖励函数，指示它当前做得是好是坏。在上面四条腿行走的例子，如果机器人向前走，激励函数可能给它正面反馈，如果退后或者迭代，激励函数给它负面反馈。如果通过选择动作获取最大的奖励自然就成了算法的任务了。

 强化学习有很多成功的应用，包括自动驾驶直升机、机器人腿定位、移动网络路由、市场策略选择、工厂控制和高效的网页索引。我们将会从马尔可夫决策过程（ MDP）的定义开始学习强化学习，它提供强化学习问题里常常拥有的形式。</code></pre>
<h2 id="马尔可夫决策过程mdp">1 马尔可夫决策过程（MDP)</h2>
<p>马尔可夫决策过程（MDP)涉及到这样一组东西 <span class="math inline">\((S,A,\{P_{sa}\},\gamma,R)\)</span></p>
<ul>
<li><p><span class="math inline">\(S\)</span> 是一组<strong>状态</strong>的集合 。（比如，在自动驾驶直升机问题上，<span class="math inline">\(S\)</span>可以是所有直升机位置和朝向的集合。分离散和连续，空间已知）</p></li>
<li><span class="math inline">\(A\)</span> 是一组<strong>动作</strong>的集合。（比如，所有你推动直升机控制杆的所有可能方向。一般是离散的，空间已知）</li>
<li><span class="math inline">\(P_{sa}\)</span>是状态转移概率 。对于每个可能的<span class="math inline">\(s\in S\)</span>和动作<span class="math inline">\(a\in A\)</span>，<span class="math inline">\(P_{sa}\)</span>是一个状态空间上的分布。详情后面我们会讨论，简单地讲，<span class="math inline">\(P_{sa}\)</span>给出如果在状态<span class="math inline">\(s\)</span>的时候我们选择动作<span class="math inline">\(a\)</span>所能达到状态的概率分布。（一般是未知的，或用经验概率近似）</li>
<li><span class="math inline">\(\gamma \in [0,1)\)</span> 称作<strong>折扣因子</strong> （超参数）。</li>
<li><p>$R:S A  $ 是<strong>奖励函数</strong> 。（有时奖励函数只写成状态的函数 $R:S  $ ，奖励函数是已知的）</p>
<p>MDP的动态过程如下：Agent（动作的执行者）从环境状态 <span class="math inline">\(s_0\)</span> 开始，从动作集合<span class="math inline">\(A\)</span>中选取一个动作<span class="math inline">\(a_0\)</span>。该动作会导致环境状态随机变化，该变化服从<span class="math inline">\(P_{sa}\)</span>这个状态转移概率分布，动作结束后我们得到新的状态<span class="math inline">\(s_1\)</span>, 然后 Agent 再选取一个动作<span class="math inline">\(a_1\)</span>，得到状态<span class="math inline">\(s_2\)</span>， Agent 接着选取<span class="math inline">\(a_2\)</span>… 就这样，我们得到一个序列：</p></li>
</ul>
<p><span class="math display">\[
s_0 \xrightarrow {\text{a0}}s_1 \xrightarrow {\text{a1}}s_2 \xrightarrow {\text{a2}}s_3 \xrightarrow {\text{a3}}...
\]</span></p>
<p>​ 遍历上面的状态<span class="math inline">\(s_0,s_1...\)</span>和动作<span class="math inline">\(a_0,a_1...\)</span>，我们定义总的收益 <span class="math display">\[
R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...
\]</span> 或者只看成状态的函数 <span class="math display">\[
R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+...
\]</span></p>
<pre><code>为了简化，后续我们只使用“状态-奖励”函数 $R(s)$，尽管泛化成“（状态-动作)-奖励”函数 $R(s,a)$ 并不十分困难。

在强化学习里，我们的目标是随着时间推进，选择的动作组能最大化总收益的期望：</code></pre>
<p><span class="math display">\[
E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+...]
\]</span> 注意到 <span class="math inline">\(t\)</span> 时刻的奖励被打了折扣 <span class="math inline">\(\gamma^t\)</span>。因此为了期望尽可能的大，我们会希望尽快获取正向的奖励（尽可能推迟负奖励的到来）。在经济领域的应用上，<span class="math inline">\(R(\cdot)\)</span> 可以是利润，此时 <span class="math inline">\(\gamma\)</span> 的一个自然释意是利率（今天的一块钱比未来的一块钱价值更高）。</p>
<pre><code>**策略**指任意状态到动作的函数映射 $\pi : S \rightarrow A$。任何时候我们说**执行**某个策略是指我们按该策略选取动作，即$a=\pi(s)$。我们同时定义一个策略的状态**值函数**根据以下方程（或者状态动作值函数）</code></pre>
<p><span class="math display">\[
V^\pi(s) = E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+...|s_0=s,\pi]
\]</span></p>
<p><span class="math inline">\(V^\pi(s)\)</span> 只是根据策略 <span class="math inline">\(\pi\)</span> 从状态<span class="math inline">\(s\)</span>开始的折扣奖励的期望和。</p>
<pre><code>换个角度，一个给定策略 $\pi$ 的值函数 $V^\pi$ 满足**贝尔曼方程组（Bellman Equations）**:</code></pre>
<p><span class="math display">\[
V^\pi(s) = R(s) + \gamma \sum_{s&#39; \in S} P_{s\pi(s)}(s&#39;)V^\pi(s&#39;)
\]</span></p>
<p>就是说从状态 <span class="math inline">\(s\)</span> 开始折扣奖励的期望和 <span class="math inline">\(V^\pi(s)\)</span> 由两项组成：第一项，<strong>即时奖励</strong> <span class="math inline">\(R(s)\)</span> 我们在开始状态 s 获得的奖励，然后，第二项，未来折扣的奖励与对应状态出现概率乘积之和。更详细地检查第二项，我们看到其中的求和项可以重写成未来状态 <span class="math inline">\(s&#39;\)</span> 其奖励之期望 <span class="math inline">\(E_{s&#39;\sim{P_{s\pi(s)}}}[V^\pi(s&#39;)]\)</span>。其中未来状态 <span class="math inline">\(s&#39;\)</span> 服从分布 <span class="math inline">\(P_{s\pi(s)}\)</span>，该分布是我们从先前状态 <span class="math inline">\(s\)</span> 依据策略执行动作 <span class="math inline">\(\pi(s)\)</span> 之后会得到的。综上，方程的第二项给出MDP第一步动作执行之后，所能获得的折扣奖励期望和。（这里的折扣奖励是指 <span class="math inline">\(\gamma V^\pi(s&#39;)\)</span> ，其实翻译成折扣状态值更恰当，因为并非由奖励函数 <span class="math inline">\(R\)</span> 给出）</p>
<pre><code>我们可以使用贝尔曼方程组高效求解值函数 $V^\pi$。特别地，在有限状态MDP中（$|S| &lt; \infty $），我们可以为 $V^\pi(s)$ 的每个状态 $s$ 写下一条方程，得到一个 $|S|$ 个变量的 $|S|$ 条方程的方程组，高效求解所有状态的值 $V^\pi(s)$。（即由于策略已确定， $V^\pi(s_i)$ 可当作变量，$R(s_i)$ 和 $P_{s_i\pi(s_i)}(s_i&#39;)$ 都是常量，其中$ 0 \le i &lt; |S|, i \in \mathbb{R}  $）

我们按下面这条公式定义**最优值函数**(即，依照该策略，每个状态都能取到最大的值)</code></pre>
<p><span class="math display">\[
V^\*(s) = \max_{\pi} V^\pi(s)  \tag{1}
\]</span></p>
<p>换句话讲，这个就是使用任意策略我们能够得到的所有可能中，折扣奖励期望和最大的。下面是贝尔曼方程组版本的最优值函数（跳过策略，直接选取最优动作，使折扣奖励期望和最大）：</p>
<p><span class="math display">\[
V^\*(s) = R(s) +  \max_{a \in A} \gamma \sum_{s&#39; \in S} P_{sa}(s&#39;)V^\*(s&#39;)  \tag{2}
\]</span></p>
<p>跟上面一样，第一项是既时的奖励。在所有可以执行的动作 <span class="math inline">\(a\)</span> 中，我们选取能是折扣奖励期望和最大的来执行，第二项是该最优动作带来的折扣奖励期望和。你应该确保自己理解该方程，思考下为什么是这样的。</p>
<pre><code>我们再定义一个策略 $\pi^\*: S \rightarrow A$，如下：</code></pre>
<p><span class="math display">\[
\pi^\*(s) = arg \max_{a \in A} \sum_{s&#39; \in S} P_{sa}(s&#39;)V^\*(s&#39;) \tag3
\]</span> 注意到， <span class="math inline">\(\pi^\*(s)\)</span> 给出的动作 <span class="math inline">\(a\)</span> 在方程(2)中的“max”里面取得最大。</p>
<pre><code>实际上，对于所有的状态和所有的策略，我们有：</code></pre>
<p><span class="math display">\[
V^{\pi^\* } (s)= V^\*(s) \geq V^\pi(s)
\]</span> 上式等号表明，对于全部状态，策略 <span class="math inline">\(\pi^\*\)</span> 的值函数 <span class="math inline">\(V^{\pi^\*}\)</span> 等于最优值函数 <span class="math inline">\(V^\*\)</span>。另外，大于等于号表明，<span class="math inline">\(\pi^\*\)</span> 策略的值至少是跟其他策略的值一样大。换句话说，方程(3)中定义的 <span class="math inline">\(\pi^\*\)</span> 是<strong>最优策略</strong>。</p>
<pre><code>值得注意的是，最优策略 $\pi^\*$ 有些有趣的性质，它对于全部状态 $s$ 是最优的。确切地说，不会出现当我们从状态 $s$ 开始，得到这个状态某个最优策略，然后从另一个状态 $s&#39;$ 开始，得到另一个最优策略。在方程(1)中，对于全部状态，同一个最优策略 $\pi^\*$ 取得最大值。这意味着不管初始状态如何，我们可以使用同一个最优策略 $\pi^\*$。</code></pre>
<h2 id="值迭代与策略迭代">2 值迭代与策略迭代</h2>
<p>下面我们描述两个解决有限状态MDPs的有效算法。现在我们只考虑状态和动作空间有限的MDPs(<span class="math inline">\(|S|&lt;\infty,|A|&lt;\infty\)</span>)。</p>
<pre><code>第一个算法，**值迭代**，如下：</code></pre>
<ol type="1">
<li><p>对于每个状态 <span class="math inline">\(s\)</span>，初始化 <span class="math inline">\(V(s):=0\)</span></p></li>
<li><p>重复直到收敛 { 对每个状态，更新 $V(s) := R(s) + max_{a A} <em>{s’}P</em>{sa}(s’)V(s’) $ } 此算法可以看成使用 贝尔曼方程 (2) 重复尝试去估计值函数。</p>
<p>有两种可能的方式来执行算法内部循环的更新操作。 第一种是，<strong>同步更新</strong>，我们先算出每个状态 <span class="math inline">\(s\)</span> 对应的新值 <span class="math inline">\(V(s)\)</span>，然后用全部的新值去覆盖掉全部的旧值。这种操作里，算法可以看成实现了把值函数的当前估计映射到新估计上的“贝尔曼拷贝操作(Ballman backup operator)”。第二种是，<strong>异步更新</strong>。我们以某种顺序便利内部循环，每次都更新一个值。</p>
<p>无论哪种，可以证明值迭代可以让 <span class="math inline">\(V\)</span> 收敛到 <span class="math inline">\(V^\*\)</span>。有了 <span class="math inline">\(V^\*\)</span> 我们可以用方程(3)得出最优策略。</p>
<p>除了值迭代，还有第二个标准算法，<strong>策略迭代</strong>，可用于找到MDPs的最优策略。算法如下：</p></li>
<li>随机初始化策略 <span class="math inline">\(\pi\)</span>。</li>
<li>重复直至收敛 {
<ol type="a">
<li>让 <span class="math inline">\(V:=V^{\pi}\)</span></li>
<li>对于每个状态 <span class="math inline">\(s\)</span>，让 <span class="math inline">\(\pi(s) := arg \max_{a \in A} \sum_{s&#39;} P_{sa}(s&#39;) V(s&#39;)\)</span>。 }</li>
</ol></li>
</ol>
<p>因此，在内部循环重复计算当前策略的值函数，然后用当前值函数更新策略（步骤(b)中找到的策略也叫关于值函数 <span class="math inline">\(V\)</span> 的贪心策略）。注意到步骤(a)可以用先前提到的求贝尔曼方程线性方程组的方式求解，对于特定的策略来讲，这只是一组拥有 <span class="math inline">\(|S|\)</span> 个变量的 <span class="math inline">\(|S|\)</span> 条方程的方程组。</p>
<pre><code>在此算法进行最多有限次数的迭代后，$V$ 将会收敛到 $V^\*$，此时对应的 $\pi$ 收敛到 $\pi^\*$ 。

值迭代跟策略迭代都是解决马尔可夫决策过程的标准算法，关于哪个算法更好现在并没有统一的定论。对于小规模的的马尔可夫决策过程，策略迭代通常很快，很少几次迭代即可收敛。不过，对于拥有较大状态空间的马尔可夫决策过程，在显式求解 $V^\pi$ 的一系列大的线性方程组上会遇到困难。在这些问题上，值迭代可能被优先考虑。因此，在实践中，似乎值迭代比策略迭代用得更频繁。</code></pre>
<h2 id="学习mdp模型">3 学习MDP模型</h2>
<p>目前为止，我们讨论的马尔可夫决策过程和求解算法都是假设状态转移概率和奖励是已知的。在很多现实问题中，没有显示的状态转移概率和奖励提供，我们需要从数据里估算它们。（通常，<span class="math inline">\(S\)</span>，<span class="math inline">\(A\)</span> 和 <span class="math inline">\(\gamma\)</span> 是已知的）</p>
<pre><code>比如，假设，对于倒立摆问题（参考问题集4），我们拿到一定数量的马尔可夫决策过程的试验，如下：</code></pre>
<p><span class="math display">\[
\begin{array}
\ s^{(1)}_0 \xrightarrow {a^{(1)}_0}s^{(1)}_1 \xrightarrow {a^{(1)}_1}s^{(1)}_2 \xrightarrow {a^{(1)}_2}s^{(1)}_3 \xrightarrow {a^{(1)}_3} \ldots \\\\
s^{(2)}_0 \xrightarrow {a^{(2)}_0}s^{(2)}_1 \xrightarrow {a^{(2)}_1}s^{(2)}_2 \xrightarrow {a^{(2)}_2}s^{(2)}_3 \xrightarrow {a^{(2)}_3} \ldots \\\\
\ldots
\end{array}
\]</span></p>
<pre><code>这里 $s_i^{(j)}$ 是我们第 $j$ 次试验在时间 $i$ 的状态。 $a_i^{(j)}$ 是在该状态我们选择的对应动作。实际上，每次试验都会一直执行，直到马尔可夫决策过程终止（比如在倒立摆问题上杆倒了），或者执行有限的次数。

有了在MDP上获得的一组试验组成的“经验”，我们可以很容易获得状态转换概率的极大似然近似估计：</code></pre>
<p><span class="math display">\[
P_{sa}(s&#39;) = \frac{\\#我们在状态{s}执行动作a到达{s&#39;}的次数}{\\#我们在状态s执行动作a的次数} \tag4
\]</span> 或者，如果上面的比例是 “0/0”——对应之前从未在状态 <span class="math inline">\(s\)</span> 选择执行动作 <span class="math inline">\(a\)</span> —— 我们可以简单地估计 $P_{sa}(s’) $ 为 <span class="math inline">\(1/|S|\)</span> 。（也就是，认为 <span class="math inline">\(P_{sa}\)</span> 在所有状态上服从均匀分布）</p>
<pre><code>注意到，如果我们在马尔可夫决策过程中获得更多的经验，这里有一个有效的方式用这些新的经验去更新我们的状态转移概率。具体，我们我们保持持有（4）中的分子跟分母的计数，然后当观察到更多的尝试时，可以简单地把新值累计进入就行。计算这些计数的比值就可以拿到我们的 $P_{sa}$ 的估计。

使用相同的过程，如果 $R$ 是未知的，我们可以拿在状态 $s$ 的的平均奖励作为状态 $s$ 的即刻奖励的期望值。

在学到MDP的一个模型后，既拥有状态转移概率和奖励函数的估计，我们可以利用值迭代或者策略迭代求解该MDP。举个例子，把模型学习跟值迭代放在一起，我们得到一个可能的算法用于求解未知状态转移概率的MDP：</code></pre>
<ol type="1">
<li><p>随机初始化策略 <span class="math inline">\(\pi\)</span></p></li>
<li><p>重复 {</p>
<pre><code>      （a） 按策略 $\pi$ 执行一定次数的试验。

      （b）使用MDP过程的累积经验，更新我们的估计 $P_{sa}$ （和 $R$， 如果适用的话）。

      （c）用估计的状态转移概率和奖励函数进行值迭代，获得一个新的值函数的估计 $V$。

      （d）用关于值函数 $V$ 贪婪的方式，更新策略 $\pi$ 。

 }</code></pre>
<p>我们注意到，对于这个特定的算法有一个简单的优化可以让它跑得更快。具体，在算法的内部循环，我们执行值迭代的地方，我们可以用上一次循环得到的结果来初始化值函数 <span class="math inline">\(V\)</span> ，取代零初始化，这会给算法一个更好的起点，收敛更快。</p></li>
</ol>
<h2 id="连续状态下的mdps">4 连续状态下的MDPs</h2>
<p>到目前为止，我们仅关注有限状态的马尔可夫决策过程。下面开始讨论适用于拥有无限状态的马尔可夫决策过程的算法。</p>
<p>举个例子，对于一辆车，我们可能用 <span class="math inline">\((x,y,\theta,\vec x, \vec y, \vec \theta)\)</span> 来表示它的状态，其中它的位置 <span class="math inline">\((x,y)\)</span>；朝向 <span class="math inline">\(\theta\)</span>；在 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 方向的速度 <span class="math inline">\(\vec x\)</span> 和 <span class="math inline">\(\vec y\)</span> ；以及角速度 <span class="math inline">\(\vec \theta\)</span>。因此，<span class="math inline">\(S = \mathbb{R}^6\)</span> 这组状态集合是无限的，因为对于车辆的位置和朝向是有无限个可能值的。</p>
<p>相似地，你在PS4上看到的倒立摆游戏有状态 <span class="math inline">\((x,\theta\,\vec x, \vec \theta)\)</span>，其中 <span class="math inline">\(\theta\)</span> 是杆的角度。又或者，在三维空间中飞行的直升飞机拥有 <span class="math inline">\((x,y,z,\phi, \theta, \psi, \vec x, \vec y, \vec z, \vec \phi, \vec \theta, \vec \psi)\)</span> 形式的状态，其中这里的 <span class="math inline">\(\phi\)</span> 、<span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\psi\)</span> 指定了直升飞机的空间朝向。</p>
<pre><code>在这个章节，我们将考虑状态空间是 $S = \mathbb{R}^n$ 的情形，然后阐述解决这类MDP的方法。</code></pre>
<h3 id="离散化">4.1 离散化</h3>
<p>将连续状态MDP问题中的状态空间离散化可能是最简单的方式了，然后用上面的值迭代或策略迭代求解。</p>
<pre><code>比如，对于二维的状态 $(s_1,s_2)$，我们可以使用格子来离散化状态空间：</code></pre>
<figure>
<img src="/assets/mdp_img1.png" alt="img1"><figcaption>img1</figcaption>
</figure>
<p>这里，每个格子表示一个单独的离散状态 <span class="math inline">\(\bar{s}\)</span> 。接着我们可以通过离散的 <span class="math inline">\((\bar{S}, A, \{P_{\bar{s}a} \}, \gamma, R )\)</span>，其中 <span class="math inline">\(\bar{S}\)</span> 是离散状态集合。<span class="math inline">\(\{ P_{\bar{s}a} \}\)</span> 是我们在离散空间上的状态转移概率。我们可以用值迭代或者策略迭代求解最优值函数 <span class="math inline">\(V^\*(\bar{s})\)</span> 或者最优策略 <span class="math inline">\(\pi^\*(\bar{s})\)</span> 。当我们的真实系统处在某个连续值状态 <span class="math inline">\(s \in S\)</span> 中时，我们需要选择一个动作去执行，我们计算对应的离散状态 <span class="math inline">\(\bar{s}\)</span>，然后执行动作 <span class="math inline">\(\pi^\*(\bar{s})\)</span> 。</p>
<pre><code>在很多问题上，离散方法是可行的。不过有两个缺点。第一，它得到的结果 $V^\*$ (或者 $\pi^\*$) 都是对实际问题相当naive的简化。确切地说，它假设值函数在每个离散的区间中得到一个固定的值。

为了更好地理解上述假设的局限，可以考虑一个监督学习问题，去拟合下图的数据集：</code></pre>
<figure>
<img src="/assets/mdp_img2.png" alt="img2"><figcaption>img2</figcaption>
</figure>
<pre><code>如你所见，线性回归应该可以很好地解决此问题。但是，如果我们对 x 轴进行离散表示，并规定每个离散区间只能对应一个固定值，我们拟合的结果看起来就像：</code></pre>
<figure>
<img src="/assets/mdp_img3.png" alt="img3"><figcaption>img3</figcaption>
</figure>
<pre><code>对于很多光滑的函数，这样每个区间有个固定值的并不能很好表示它们。如此一来结果会很粗糙，在不同的区间泛化效果也不理想。用这样的方式，我们可能需要很细微的颗粒度（每个格子都足够小）才能得到一个比较好的近似结果。

另一个缺点是这样的表示会导致纬度诅咒。假设 $S = \mathbb{R}^n$ ，我们把这 n 维状态每个都离散成 k 个值。这时我们得到的状态空间值的总个数是 $k^n$ 。总个数是随 n 指数级增长的，因此无法很好地适用大规模的问题。例如，有个10维的状态，如果我们把每个纬度都离散成100个值，我们有 $100^{10} = 10^{20} $个状态值，对于现在的桌面计算机来说太大了。

根据经验，对于1维、2维的问题离散方法能很好地工作（而且有简单易于实现的优点）。也许有一些更聪明的方法，更小心地选择离散的方式，离散方法在3维、4维的问题上也能行。如果你特别聪明，然后又特别地幸运，你说不定可以用它解决6维的问题。但是更高纬度的问题，可能就无能为力了。</code></pre>
<h3 id="值函数近似">4.2 值函数近似</h3>
<p>我们现在讲一种在连续状态下寻找马尔可夫决策过程最佳策略的替代方法，我们不用离散而是直接近似 <span class="math inline">\(V^\*\)</span> 。值函数近似的方法已经在很多强化学习的问题中有成功的例子。</p>
<h4 id="使用模型或模拟器">4.2.1 使用模型或模拟器</h4>
<p>去开发值函数近似的方法，我们会先假设有一个<strong>模型</strong>或者<strong>模拟器</strong>。简单地讲，我们可以将模拟器堪称一个黑盒，它接受一个任意状态的值（连续值） <span class="math inline">\(s_t\)</span> 和动作 <span class="math inline">\(a_t\)</span>，然后根据状态转移概率 <span class="math inline">\(P_{s_t a_t}\)</span> 返回下一个状态 <span class="math inline">\(s_{t+1}\)</span> 。</p>
<figure>
<img src="/assets/mdp_img4.png" alt="img4"><figcaption>img4</figcaption>
</figure>
<pre><code>有很多途径可以得到这样一个模型，之一是使用物理模拟器。比如，在倒立摆问题中，已知 $t$ 时刻的状态和选择执行的动作 $a$，假设我们知道整个系统的参数（摆杆的长度、质量等），模拟器使用物理定律计算小车／摆杆在时刻 $t + 1$ 的位置和朝向。使用现成的软件包，提供机械系统的完整描述、当前状态 $s_t$ 和动作 $a_t$，我们可以计算未来很短时间内的状态 $s_{t+1}$ 。</code></pre>
<p>另一种方式是从马尔可夫决策过程中收集数据学习，得到一个模型。比如我们进行了 <span class="math inline">\(m\)</span> 次<strong>试验</strong>，每次试验里我们都重复选取动作执行 <span class="math inline">\(T\)</span> 个时刻。选取动作可以是随机、通过某种策略或通过其他途径。然后我们观察到如下 m 个状态序列：</p>
<p><span class="math display">\[
\begin{array}
\ s^{(1)}\_0 \xrightarrow {a^{(1)}\_0}s^{(1)}\_1 \xrightarrow {a^{(1)}\_1}s^{(1)}\_2 \xrightarrow  {a^{(1)}\_2} \\ldots  \xrightarrow {a^{(1)}\_{T-1}} s^{(1)}\_T \\\\
\ s^{(2)}\_0 \xrightarrow {a^{(2)}\_0}s^{(2)}\_1 \xrightarrow {a^{(2)}\_1}s^{(2)}\_2 \xrightarrow  {a^{(2)}\_2} \\ldots  \xrightarrow {a^{(2)}\_{T-1}} s^{(2)}\_T \\\\
\ldots \\\\
\ s^{(m)}\_0 \xrightarrow {a^{(m)}\_0}s^{(m)}\_1 \xrightarrow {a^{(m)}\_1}s^{(m)}\_2 \xrightarrow  {a^{(m)}\_2} \\ldots  \xrightarrow {a^{(m)}\_{T-1}} s^{(m)}\_T \\\\
\end{array}
\]</span></p>
<p>我们可以把 <span class="math inline">\(s_{t+1}\)</span> 作为 <span class="math inline">\(s_t\)</span> 和 <span class="math inline">\(a_t\)</span> 的函数，用学习算法预测它。</p>
<pre><code>比如，某人可能选择学习一个跟线性回归相似的线性模型</code></pre>
<p><span class="math display">\[
s_{t+1}= As_t + Ba_t \tag 5
\]</span> 这里，模型的参数是矩阵 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> ，可以用上面收集的数据估计它们 <span class="math display">\[
arg \min_{A,B} \sum_{i=1}^m \sum_{t=0}^{T-1}||s_{t+1}^{(i)} - (As_t^{(i)} + Ba_t^{(i)})||
\]</span></p>
<p>(这里对应参数的极大似然估计)</p>
<pre><code>学到 $A$ 和 $B$ 之后，可以建立一个**确定**模型，在确定模型中，给定 $s_t$ 和 $a_t$ 输入 $s_{t+1}$ 是固定的。确切地讲，我们总是通过（5）计算 $s_{t+1}$ 。另外我们也可以建立一个**随机**模型，将 $s_{t+1}$ 作为输入的随机函数，建模如下：</code></pre>
<p><span class="math display">\[
s_{t+1}  = As_t + Ba_t + \epsilon_t
\]</span> 其中 <span class="math inline">\(\epsilon_t\)</span> 是噪音项，通常取 <span class="math inline">\(\epsilon_t \cal {\sim} N( \rm 0, Σ)\)</span> 。（其中 协方差矩阵 <span class="math inline">\(Σ\)</span> 也可以用收集的数据直接估计 ）</p>
<pre><code>这里，我们把下一个状态 $s_{t+1}$ 写成当前状态和动作的线性函数；当然，非线性函数也是可行的。确切地，可以学习一个模型 $s_{t+1} = A\phi_s{s_t} + B\phi_a(a_t)$ ，其中 $\phi_s$ 和 $\phi_a$是状态和动作的非线性映射。另外也可以使用非线性算法，比如局部权重线性回归，把 $s_{t+1} $ 作为 $s_t$ 和 $a_t$ 的函数来学习估计。用这些方法都可以建立确定或随机模型。</code></pre>
<h4 id="拟合值迭代">4.2.2 拟合值迭代</h4>
<p>现在我们描述用于近似连续状态MDP过程中值函数的<strong>拟合值迭代</strong>算法。方便继续，我们假设问题处在一个连续的状态空间 <span class="math inline">\(S = \mathbb{R}^n\)</span> ，但是动作空间 <span class="math inline">\(A\)</span> 很小而且是离散的。</p>
<pre><code>回想一下，在值迭代中，我们执行更新</code></pre>
<p><span class="math display">\[
V(s) := R(s) + \lambda \max_a\int P_{sa}(s&#39;)V(s&#39;)ds&#39; \tag 6 \\
\]</span></p>
<p><span class="math display">\[
= R(s)+ \lambda \max_a E_{s&#39; \sim P_{sa} } [ V(s&#39;)] \tag7
\]</span></p>
<p>(在第2章节中，我们是写成求和的形式，而不是这里的积分形式；新的记号法表示我们正在处理连续的状态空间)</p>
<pre><code>拟合值迭代的主要想法是在有限的状态样本 $s^{(1)}, \\dots, s^{(m)}$上，近似地计算上面的更新值。在下面我们将使用监督学习的算法——线性回归——来近似值函数，此时将值函数看作是状态的线性或非线性函数：</code></pre>
<p><span class="math display">\[
V(s) = \theta^T \phi(s)
\]</span> 这里的 <span class="math inline">\(\phi\)</span> 是状态的某个适当映射。</p>
<pre><code>对于每个状态值 $s(i)$ ，拟合值迭代会先计算一个量 $y(i)$ ，这个我们对 $ R(s)+ \lambda \max_a E_{s&#39; \sim P_{sa} } [ V(s&#39;)]  $ 的近似（方程7的右手边）。然后它用监督学习的方式去尝试让 $V(s)$ 靠近 $ R(s)+ \lambda \max_a E_{s&#39; \sim P_{sa} } [ V(s&#39;)] $ (换句话说，让 $V(s) $ 靠近 $y(i)$)  。</code></pre>
<p>具体算法如下：</p>
<ol type="1">
<li><p>随机取 m 个状态样本 <span class="math inline">\(s^{(1)}, \\dots, s^{(m)} \in S\)</span></p></li>
<li><p>初始化 <span class="math inline">\(\theta := 0\)</span></p></li>
<li><p>重复 {</p>
<p>对于 $i = 1, \dots, m $ {</p>
<p>​   对于每个动作 <span class="math inline">\(a \in A\)</span> {</p>
<p>    取样 $s’, \dots, s’_k P_{s^{(i)}a} $ (使用MDP模型)</p>
<p>    让 $ q(a) = 1/k ^k_{j=1} (R(s^{(i)}) + V(s’_j))$     // 也就是, $ R(s)+ E_{s’ P_{sa} } [ V(s’)] $</p>
<pre><code>}</code></pre>
<p>​ 令 <span class="math inline">\(y(i) = max_a q(a)\)</span></p>
<pre><code>  //  也就是， $ R(s)+ \lambda \max_a E_{s&#39; \sim P_{sa} } [ V(s&#39;)]  $</code></pre>
<p>​ }</p>
<p>​ // 在原始版本的值迭代算法（离散状态上的）</p>
<p>​ // 我们根据 <span class="math inline">\(V(s^{(i)}) : = y^{(i)}\)</span> 更新值函数</p>
<p>​ // 在这个算法，我们通过监督学习（线性回归)</p>
<p>​ // 达到我们想要的 <span class="math inline">\(V(s^{(i)}) \approx y^{(i)}\)</span></p>
<p>​ 让 <span class="math inline">\(\theta : = arg \min_\theta 1/2 \sum_{i=1}^m( \theta^T \phi(s^{(i)}) - y^{(i)} )^2\)</span></p>
<p>}</p>
<p>上述，我们使用线性回归写出拟合值迭代算法，尝试让 $V(s^{(i)}) $ 靠近 <span class="math inline">\(y^{(i)}\)</span> 。这一步跟标准的监督学习完全相似，在监督学习里我们有一个训练集 $(x^1, y<sup>1),(x</sup>2, y<sup>2),(x</sup>3, y^3) , \dots, (x^m, y^m) $， 想学习一个 x 到 y 的函数映射；唯一的区别是这里 s 取代了 x。 虽然上面我们用了线性回归，但其他回归算法也是可行的（比如，局部权重线性回归）。</p>
<p>跟值迭代不一样的是，拟合值迭代并不能被证明总是收敛的。不过，在实际应用过程中，它确实经常收敛（或接近收敛），而且在很多问题上表现良好。值得注意的是，如果我们使用确定性的MDP模拟器或模型，拟合值迭代可以简化到 <span class="math inline">\(k = 1\)</span> 的形式。这时因为在确定性模型中，一个样本已经足够获取奖励的期望。对于非确定性模型，我们需要 k 个样本，用它们的均值来近似期望（注意算法伪代码里$ q(a) $的定义）。</p>
<p>最后，拟合值迭代给出的值函数 V 近似最优值函数 <span class="math inline">\(V^\*\)</span> 。这里隐式子定义了我们的策略。具体地，当我们的系统处在状态 <span class="math inline">\(s\)</span> 时，我们根据下面式子选择动作 <span class="math inline">\(a\)</span> <span class="math display">\[
arg \max_a E_{s&#39; \sim P_{sa}}[V(s&#39;)] \tag 8
\]</span> 这个计算过程跟拟合值÷迭代算法的内部循环相似，对于每个动作，我们采集 k 个样本 <span class="math inline">\(s&#39;\_1,s&#39;\_2, \\dots, s&#39;\_k \sim P_{sa}\)</span> 来近似地计算期望。（再次提醒，如果是确定性模型，让 k =1 ）</p>
<p>实际操作中，也有其他的方法来近似这一步。比如，一个非常普遍的例子，如果我们的模拟器的形式是 <span class="math inline">\(s_{t+1} = f(s_t,a_t) + \epsilon_t\)</span> ，其中 <span class="math inline">\(f\)</span> 是某个确定函数（比如， <span class="math inline">\(f(s_t,a_t) = As_t + ba_t\)</span>），<span class="math inline">\(\epsilon_t\)</span> 是高斯噪音。在这个例子，我们可以选取动作 <span class="math display">\[
arg \max_a(V(f(s,a)))
\]</span> 换句话说，我们简单的忽略掉噪音项，让 <span class="math inline">\(\epsilon = 0\)</span>， <span class="math inline">\(k = 1\)</span>。 等价地，这个可以用方程8推导出来 <span class="math display">\[
E_{s&#39;}(V(s&#39;)) \approx V(E_{s&#39;}(s&#39;)) \tag 9 \\
\]</span> <span class="math display">\[
= V(f(s,a)) \tag {10}
\]</span></p></li>
</ol>
<p>其中这里的期望是在随机的 <span class="math inline">\(s&#39; \sim P_{sa}\)</span> 上的。所以只要噪音项不是很大，这通常是个合理的近似。</p>
<pre><code>但是，对于不能使用该近似方法的问题，我们不得不去采集 $k|A|$ 个样本（使用模型），去近似上面的期望，这样的计算量会大很多。</code></pre>
<p>原文 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf" target="_blank" rel="noopener">CS229 Lecture notes Part XIII</a> 是该课程的一部分<a href="http://cs229.stanford.edu/syllabus.html" target="_blank" rel="noopener">CS229 lecture 16 - 18</a> 视频（油管）<a href="https://www.youtube.com/watch?v=RtxI449ZjSc" target="_blank" rel="noopener">CS229 Lecture by Andrew Ng</a></p>
<blockquote>
<p>译者注：懒癌发作，翻译完成于 2018年八月20号 上午 11:33，历时一个月。</p>
</blockquote>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jjjeaswn Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Jjjeaswn" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:wangzixin1113@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#part-iii-强化学习与控制"><span class="nav-number">1.</span> <span class="nav-text">Part III 强化学习与控制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔可夫决策过程mdp"><span class="nav-number">1.1.</span> <span class="nav-text">1 马尔可夫决策过程（MDP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#值迭代与策略迭代"><span class="nav-number">1.2.</span> <span class="nav-text">2 值迭代与策略迭代</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习mdp模型"><span class="nav-number">1.3.</span> <span class="nav-text">3 学习MDP模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#连续状态下的mdps"><span class="nav-number">1.4.</span> <span class="nav-text">4 连续状态下的MDPs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#离散化"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 离散化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#值函数近似"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 值函数近似</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用模型或模拟器"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">4.2.1 使用模型或模拟器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拟合值迭代"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">4.2.2 拟合值迭代</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jjjeaswn Wong</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.4.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  



  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
